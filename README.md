# LLMScan
A research-oriented system that automatically detects unsafe or unreliable  outputs generated by Large Language Models (LLMs).
